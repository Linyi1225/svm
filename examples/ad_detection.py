"""
å¹¿å‘Šæ£€æµ‹å®Œæ•´ç¤ºä¾‹ - æ”¹è¿›ç‰ˆ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from svm import GaussianSVM, generate_ad_detection_data, plot_decision_boundary, plot_classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

def main():
    print("ğŸ¯ å¹¿å‘Šæ£€æµ‹SVMç¤ºä¾‹")
    print("=" * 40)
    
    # ç”Ÿæˆå¹¿å‘Šæ£€æµ‹æ•°æ®ï¼ˆæ³¨æ„ï¼šè¿™é‡Œç”Ÿæˆçš„æ•°æ®å·²ç»æ ‡å‡†åŒ–äº†ï¼‰
    print("ğŸ“Š ç”Ÿæˆå¹¿å‘Šæ£€æµ‹æ•°æ®é›†...")
    X, y = generate_ad_detection_data(n_samples=400)
    
    feature_names = ['ç‚¹å‡»ç‡', 'åœç•™æ—¶é—´', 'å…³é”®è¯å¯†åº¦', 'ä»·æ ¼æ•æ„Ÿåº¦']
    print(f"ç‰¹å¾: {feature_names}")
    print(f"å¹¿å‘Šæ ·æœ¬: {np.sum(y == 1)}")
    print(f"æ­£å¸¸å†…å®¹: {np.sum(y == -1)}")
    
    # æ•°æ®åˆ’åˆ†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: å¹¿å‘Š={np.sum(y_train == 1)}, å†…å®¹={np.sum(y_train == -1)}")
    
    # è®­ç»ƒSVM - è°ƒæ•´å‚æ•°ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
    print("\nğŸš€ è®­ç»ƒå¹¿å‘Šæ£€æµ‹SVM...")
    svm = GaussianSVM(C=1.0, gamma=0.5, max_iter=300)  # é™ä½å¤æ‚åº¦
    svm.fit(X_train, y_train)
    
    # è¯„ä¼°æ€§èƒ½
    print("\nğŸ“ˆ æ€§èƒ½è¯„ä¼°:")
    train_report = svm.get_classification_report(X_train, y_train)
    test_report = svm.get_classification_report(X_test, y_test)
    
    print("è®­ç»ƒé›†è¡¨ç°:")
    print(f"  å‡†ç¡®ç‡: {train_report['accuracy']:.3f}")
    print(f"  ç²¾ç¡®ç‡: {train_report['precision']:.3f}")
    print(f"  å¬å›ç‡: {train_report['recall']:.3f}")
    print(f"  F1åˆ†æ•°: {train_report['f1_score']:.3f}")
    
    print("æµ‹è¯•é›†è¡¨ç°:")
    print(f"  å‡†ç¡®ç‡: {test_report['accuracy']:.3f}")
    print(f"  ç²¾ç¡®ç‡: {test_report['precision']:.3f}")
    print(f"  å¬å›ç‡: {test_report['recall']:.3f}")
    print(f"  F1åˆ†æ•°: {test_report['f1_score']:.3f}")
    
    # å¯è§†åŒ–ï¼ˆä½¿ç”¨å‰ä¸¤ä¸ªç‰¹å¾ï¼‰
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
    plot_decision_boundary(svm, X_train, y_train)
    plot_classification_report(svm, X_test, y_test)
    
    # å®é™…æ¡ˆä¾‹é¢„æµ‹ - é‡æ–°ç”ŸæˆåŸå§‹æ•°æ®å¹¶æ­£ç¡®å¤„ç†æ ‡å‡†åŒ–
    print("\nğŸ” å®é™…æ¡ˆä¾‹é¢„æµ‹:")
    
    # é‡æ–°ç”ŸæˆåŸå§‹æ•°æ®ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰æ¥åˆ›å»ºæ­£ç¡®çš„scaler
    print("é‡æ–°ç”ŸæˆåŸå§‹æ•°æ®ä»¥è·å–æ­£ç¡®çš„æ ‡å‡†åŒ–å™¨...")
    X_raw, y_raw = generate_raw_ad_detection_data(n_samples=1000)
    
    # ç”¨åŸå§‹æ•°æ®è®­ç»ƒæ ‡å‡†åŒ–å™¨
    scaler = StandardScaler()
    scaler.fit(X_raw)
    
    # æ„é€ æµ‹è¯•æ¡ˆä¾‹ï¼ˆä½¿ç”¨åŸå§‹ç‰¹å¾å€¼ï¼‰
    test_cases = [
        {
            'name': 'å¯ç–‘å¹¿å‘Š',
            'features': [0.12, 25, 0.15, 0.85],  # é«˜ç‚¹å‡»ç‡ï¼ŒçŸ­åœç•™ï¼Œé«˜å…³é”®è¯å¯†åº¦ï¼Œé«˜ä»·æ ¼æ•æ„Ÿåº¦
            'description': 'é«˜ç‚¹å‡»ç‡ï¼ŒçŸ­åœç•™æ—¶é—´ï¼Œé«˜å…³é”®è¯å¯†åº¦ï¼Œé«˜ä»·æ ¼æ•æ„Ÿåº¦'
        },
        {
            'name': 'ä¼˜è´¨å†…å®¹',
            'features': [0.025, 180, 0.01, 0.05],  # é€‚ä¸­ç‚¹å‡»ç‡ï¼Œé•¿åœç•™ï¼Œä½å…³é”®è¯å¯†åº¦ï¼Œä½ä»·æ ¼æ•æ„Ÿåº¦
            'description': 'é€‚ä¸­ç‚¹å‡»ç‡ï¼Œé•¿åœç•™æ—¶é—´ï¼Œä½å…³é”®è¯å¯†åº¦ï¼Œä½ä»·æ ¼æ•æ„Ÿåº¦'
        },
        {
            'name': 'æ¨¡ç³Šæ¡ˆä¾‹',
            'features': [0.06, 80, 0.05, 0.3],  # ä¸­ç­‰å„é¡¹æŒ‡æ ‡
            'description': 'å„é¡¹æŒ‡æ ‡éƒ½åœ¨ä¸­ç­‰æ°´å¹³'
        },
        {
            'name': 'å…¸å‹å†…å®¹',
            'features': [0.02, 200, 0.005, 0.02],  # æ›´å…¸å‹çš„å†…å®¹ç‰¹å¾
            'description': 'ä½ç‚¹å‡»ç‡ï¼Œé•¿åœç•™æ—¶é—´ï¼Œæä½å…³é”®è¯å¯†åº¦ï¼Œæä½ä»·æ ¼æ•æ„Ÿåº¦'
        }
    ]
    
    for case in test_cases:
        # æ ‡å‡†åŒ–ç‰¹å¾
        features_scaled = scaler.transform([case['features']])
        prediction = svm.predict(features_scaled)[0]
        probabilities = svm.predict_proba(features_scaled)[0]
        confidence = max(probabilities)
        
        result = "å¹¿å‘Š" if prediction > 0 else "æ­£å¸¸å†…å®¹"
        
        print(f"\n{case['name']}:")
        print(f"  ç‰¹å¾: {case['description']}")
        print(f"  åŸå§‹å€¼: {case['features']}")
        print(f"  æ ‡å‡†åŒ–å: {features_scaled[0]}")
        print(f"  é¢„æµ‹ç»“æœ: {result}")
        print(f"  ç½®ä¿¡åº¦: {confidence:.3f}")
        print(f"  å†³ç­–åˆ†æ•°: {svm.decision_function(features_scaled)[0]:.3f}")

    # å‚æ•°æ•æ„Ÿæ€§åˆ†æ
    print("\nğŸ”§ å‚æ•°æ•æ„Ÿæ€§åˆ†æ:")
    test_params = [
        (0.1, 0.1), (0.1, 1.0), (1.0, 0.1), (1.0, 1.0), (10.0, 0.1), (10.0, 1.0)
    ]
    
    for C, gamma in test_params:
        svm_test = GaussianSVM(C=C, gamma=gamma, max_iter=100)
        svm_test.fit(X_train, y_train)
        acc = svm_test.score(X_test, y_test)
        print(f"  C={C}, Î³={gamma}: æµ‹è¯•å‡†ç¡®ç‡={acc:.3f}")
    
    # åˆ†ææ¨¡å‹ç‰¹å¾é‡è¦æ€§
    print("\nğŸ”¬ ç‰¹å¾åˆ†æ:")
    print("é€šè¿‡æ”¯æŒå‘é‡åˆ†ææ¨¡å‹å…³æ³¨çš„ç‰¹å¾æ¨¡å¼...")
    
    if hasattr(svm, 'support_vectors_'):
        sv_mean = np.mean(svm.support_vectors_, axis=0)
        sv_std = np.std(svm.support_vectors_, axis=0)
        
        print("æ”¯æŒå‘é‡ç‰¹å¾ç»Ÿè®¡ (æ ‡å‡†åŒ–å):")
        for i, feature in enumerate(feature_names):
            print(f"  {feature}: å‡å€¼={sv_mean[i]:.3f}, æ ‡å‡†å·®={sv_std[i]:.3f}")
        
        # åˆ†æä¸åŒç±»åˆ«çš„æ”¯æŒå‘é‡
        pos_sv = svm.support_vectors_[svm.support_vector_labels_ == 1]
        neg_sv = svm.support_vectors_[svm.support_vector_labels_ == -1]
        
        if len(pos_sv) > 0 and len(neg_sv) > 0:
            print("\næ­£ç±»æ”¯æŒå‘é‡ç‰¹å¾å‡å€¼:")
            pos_mean = np.mean(pos_sv, axis=0)
            for i, feature in enumerate(feature_names):
                print(f"  {feature}: {pos_mean[i]:.3f}")
                
            print("è´Ÿç±»æ”¯æŒå‘é‡ç‰¹å¾å‡å€¼:")
            neg_mean = np.mean(neg_sv, axis=0)
            for i, feature in enumerate(feature_names):
                print(f"  {feature}: {neg_mean[i]:.3f}")

def generate_raw_ad_detection_data(n_samples=300, random_state=42):
    """
    ç”ŸæˆåŸå§‹ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰çš„å¹¿å‘Šæ£€æµ‹æ•°æ®
    """
    if random_state is not None:
        np.random.seed(random_state)
    
    # æ­£å¸¸å†…å®¹ (éå¹¿å‘Š, y = -1)
    normal_clickrate = np.random.normal(0.03, 0.01, n_samples//2)
    normal_dwelltime = np.random.normal(120, 30, n_samples//2)
    normal_keyword_density = np.random.normal(0.02, 0.008, n_samples//2)
    normal_price_sensitivity = np.random.normal(0.1, 0.05, n_samples//2)

    # æ·»åŠ éçº¿æ€§ç›¸å…³æ€§
    quality_boost = np.random.normal(0, 0.5, n_samples//2)
    normal_clickrate += quality_boost * 0.01
    normal_dwelltime += quality_boost * 20

    X_normal = np.column_stack([normal_clickrate, normal_dwelltime,
                               normal_keyword_density, normal_price_sensitivity])
    y_normal = -np.ones(n_samples//2)

    # å¹¿å‘Šå†…å®¹ (å¹¿å‘Š, y = 1)
    ad_clickrate = np.random.normal(0.08, 0.02, n_samples//2)
    ad_dwelltime = np.random.normal(45, 15, n_samples//2)
    ad_keyword_density = np.random.normal(0.08, 0.02, n_samples//2)
    ad_price_sensitivity = np.random.normal(0.6, 0.15, n_samples//2)

    # æ·»åŠ å¹¿å‘Šç‰¹æœ‰çš„éçº¿æ€§æ¨¡å¼
    promo_factor = np.random.normal(0, 1, n_samples//2)
    ad_keyword_density += np.abs(promo_factor) * 0.03
    ad_price_sensitivity += np.abs(promo_factor) * 0.2

    X_ads = np.column_stack([ad_clickrate, ad_dwelltime,
                            ad_keyword_density, ad_price_sensitivity])
    y_ads = np.ones(n_samples//2)

    # åˆå¹¶æ•°æ®
    X = np.vstack([X_normal, X_ads])
    y = np.hstack([y_normal, y_ads])

    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ­£å€¼ï¼Œä½†ä¸æ ‡å‡†åŒ–
    X = np.clip(X, 0.001, None)
    
    # æ‰“ä¹±æ•°æ®
    indices = np.random.permutation(len(X))
    X = X[indices]
    y = y[indices]

    return X, y

if __name__ == "__main__":
    main()
